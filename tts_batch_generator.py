"""
æ•°å­—äººè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ - CosyVoice3ç‰ˆæœ¬
ä½¿ç”¨é˜¿é‡Œå¼€æºçš„ Fun-CosyVoice3-0.5B æ‰¹é‡ç”ŸæˆéŸ³é¢‘

ä½¿ç”¨æ–¹æ³•:
1. å®‰è£…ä¾èµ–: pip install modelscope torchaudio
2. ä¸‹è½½æ¨¡å‹: è‡ªåŠ¨ä»modelscopeä¸‹è½½ Fun-CosyVoice3-0.5B
3. å‡†å¤‡å‚è€ƒéŸ³é¢‘: å°†"å°O"çš„éŸ³é¢‘æ ·æœ¬æ”¾åœ¨ ./reference_audio/prompt.wav
4. è¿è¡Œ: python tts_batch_generator.py

æ¨¡å¼é€‰æ‹©:
- instruct2 (æ¨è): é€šè¿‡æŒ‡ä»¤æ§åˆ¶æƒ…ç»ªã€è¯­é€Ÿï¼Œéœ€è¦å‚è€ƒéŸ³é¢‘
- zero_shot: çº¯éŸ³è‰²å…‹éš†ï¼Œéœ€è¦å‚è€ƒéŸ³é¢‘
"""

import json
import os
from pathlib import Path
from typing import List, Dict
import torchaudio
from modelscope import AutoModel

class CosyVoice3Generator:
    def __init__(self,
                 output_dir="training_data",
                 model_dir='pretrained_models/Fun-CosyVoice3-0.5B',
                 prompt_audio='./reference_audio/prompt.wav',
                 mode='instruct2'):
        """
        åˆå§‹åŒ–CosyVoice3ç”Ÿæˆå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            model_dir: æ¨¡å‹è·¯å¾„ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
            prompt_audio: å‚è€ƒéŸ³é¢‘è·¯å¾„ï¼ˆ"å°O"çš„éŸ³é¢‘æ ·æœ¬ï¼‰
            mode: ç”Ÿæˆæ¨¡å¼ ('instruct2' æˆ– 'zero_shot')
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "audio").mkdir(exist_ok=True)
        (self.output_dir / "metadata").mkdir(exist_ok=True)

        self.prompt_audio = prompt_audio
        self.mode = mode

        # æ£€æŸ¥å‚è€ƒéŸ³é¢‘æ˜¯å¦å­˜åœ¨
        if not os.path.exists(prompt_audio):
            print(f"\nâš ï¸  è­¦å‘Š: å‚è€ƒéŸ³é¢‘ä¸å­˜åœ¨: {prompt_audio}")
            print("è¯·å‡†å¤‡ä¸€æ®µ'å°O'çš„éŸ³é¢‘æ ·æœ¬ï¼ˆ10-30ç§’ï¼‰ï¼Œä¿å­˜ä¸º:")
            print(f"  {prompt_audio}")
            print("\nå¦‚æœæ²¡æœ‰å‚è€ƒéŸ³é¢‘ï¼Œå°†ä½¿ç”¨é»˜è®¤éŸ³è‰²")
            self.prompt_audio = None

        # åˆå§‹åŒ–æ¨¡å‹
        print(f"\næ­£åœ¨åŠ è½½ CosyVoice3 æ¨¡å‹...")
        print(f"æ¨¡å‹è·¯å¾„: {model_dir}")
        print(f"å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œå°†è‡ªåŠ¨ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼ˆçº¦500MBï¼‰")

        self.cosyvoice = AutoModel(model_dir=model_dir)
        self.sample_rate = self.cosyvoice.sample_rate

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼é‡‡æ ·ç‡: {self.sample_rate}Hz")
        print(f"âœ… ä½¿ç”¨æ¨¡å¼: {mode}")

        # æƒ…ç»ªæ˜ å°„åˆ°æŒ‡ä»¤
        self.emotion_to_instruct = {
            'neutral': 'è¯·ç”¨å¹³é™è‡ªç„¶çš„è¯­æ°”è¯´è¯ã€‚',
            'happy': 'è¯·ç”¨å¼€å¿ƒã€æ„‰å¿«ã€å…´å¥‹çš„è¯­æ°”è¯´è¯ã€‚',
            'sad': 'è¯·ç”¨æ‚²ä¼¤ã€ä½è½ã€éš¾è¿‡çš„è¯­æ°”è¯´è¯ã€‚',
            'angry': 'è¯·ç”¨æ„¤æ€’ã€ç”Ÿæ°”ã€æ¿€åŠ¨çš„è¯­æ°”è¯´è¯ã€‚',
            'surprised': 'è¯·ç”¨æƒŠè®¶ã€åƒæƒŠã€ä¸å¯æ€è®®çš„è¯­æ°”è¯´è¯ã€‚',
            'thoughtful': 'è¯·ç”¨æ€è€ƒã€æ²‰æ€ã€çŠ¹è±«çš„è¯­æ°”è¯´è¯ã€‚',
            'fearful': 'è¯·ç”¨å®³æ€•ã€ç´§å¼ ã€æ‹…å¿ƒçš„è¯­æ°”è¯´è¯ã€‚',
            'tired': 'è¯·ç”¨ç–²æƒ«ã€æ— å¥ˆã€æ‡’æ•£çš„è¯­æ°”è¯´è¯ã€‚',
            'gentle': 'è¯·ç”¨æ¸©æŸ”ã€æŸ”å’Œã€è½»å£°ç»†è¯­çš„è¯­æ°”è¯´è¯ã€‚',
            'confident': 'è¯·ç”¨è‡ªä¿¡ã€åšå®šã€æœ‰åŠ›çš„è¯­æ°”è¯´è¯ã€‚',
            'professional': 'è¯·ç”¨ä¸“ä¸šã€æ­£å¼ã€ä¸¥è‚ƒçš„è¯­æ°”è¯´è¯ã€‚',
            'casual': 'è¯·ç”¨éšæ„ã€è½»æ¾ã€èŠå¤©çš„è¯­æ°”è¯´è¯ã€‚',
            'curious': 'è¯·ç”¨å¥½å¥‡ã€ç–‘é—®ã€è¯¢é—®çš„è¯­æ°”è¯´è¯ã€‚',
            'storytelling': 'è¯·ç”¨è®²æ•…äº‹èˆ¬ç”ŸåŠ¨ã€æœ‰è¶£ã€æŠ“äººçš„è¯­æ°”è¯´è¯ã€‚'
        }

    def generate_training_set(self, script_json_path="training_script.json"):
        """ç”Ÿæˆå®Œæ•´è®­ç»ƒé›†"""

        # ä»JSONæ–‡ä»¶åŠ è½½æ–‡æ¡ˆ
        with open(script_json_path, 'r', encoding='utf-8') as f:
            script_sections = json.load(f)

        # ç”ŸæˆéŸ³é¢‘
        all_metadata = []
        audio_id = 0

        print(f"\nå¼€å§‹ç”ŸæˆéŸ³é¢‘...")
        print(f"æ€»sectionæ•°: {len(script_sections)}")

        for section_idx, section in enumerate(script_sections, 1):
            print(f"\n[{section_idx}/{len(script_sections)}] ç”Ÿæˆ [{section['section']}]")

            for sentence in section['sentences']:
                audio_id += 1

                # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å
                audio_filename = f"audio_{audio_id:04d}.wav"
                audio_path = self.output_dir / "audio" / audio_filename

                # ç”ŸæˆéŸ³é¢‘
                print(f"  {audio_id:04d}. {sentence[:30]}{'...' if len(sentence) > 30 else ''}")

                try:
                    audio_tensor = self.generate_audio(
                        text=sentence,
                        emotion=section['emotion']
                    )

                    # ä¿å­˜éŸ³é¢‘
                    torchaudio.save(str(audio_path), audio_tensor, self.sample_rate)

                    # è®¡ç®—æ—¶é•¿
                    duration = audio_tensor.shape[1] / self.sample_rate

                    # è®°å½•å…ƒæ•°æ®
                    metadata = {
                        "id": audio_id,
                        "filename": audio_filename,
                        "text": sentence,
                        "section": section['section'],
                        "emotion": section['emotion'],
                        "duration": round(duration, 2),
                        "sample_rate": self.sample_rate
                    }
                    all_metadata.append(metadata)

                except Exception as e:
                    print(f"    âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
                    continue

        # ä¿å­˜å…ƒæ•°æ®ç´¢å¼•
        metadata_path = self.output_dir / "metadata" / "index.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)

        # ç»Ÿè®¡ä¿¡æ¯
        total_duration = sum(m['duration'] for m in all_metadata)

        print(f"\n{'='*60}")
        print(f"âœ… å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ç”ŸæˆéŸ³é¢‘æ•°: {len(all_metadata)}")
        print(f"æ€»æ—¶é•¿: {total_duration/60:.1f} åˆ†é’Ÿ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"å…ƒæ•°æ®: {metadata_path}")

        return all_metadata

    def generate_audio(self, text: str, emotion: str = "neutral"):
        """
        ä½¿ç”¨CosyVoice3ç”Ÿæˆå•å¥éŸ³é¢‘

        Args:
            text: è¦ç”Ÿæˆçš„æ–‡æœ¬
            emotion: æƒ…ç»ªæ ‡ç­¾

        Returns:
            audio_tensor: torch.Tensor, shape (1, samples)
        """
        # æ„å»ºæŒ‡ä»¤
        instruct = self.emotion_to_instruct.get(emotion, self.emotion_to_instruct['neutral'])
        prompt_text = f"You are a helpful assistant. {instruct}<|endofprompt|>"

        if self.mode == 'instruct2':
            # ä½¿ç”¨instruct2æ¨¡å¼ï¼ˆæ¨èï¼‰
            output = None
            for i, j in enumerate(self.cosyvoice.inference_instruct2(
                text,
                prompt_text,
                self.prompt_audio if self.prompt_audio else './asset/zero_shot_prompt.wav',
                stream=False
            )):
                output = j['tts_speech']
                break  # åªå–ç¬¬ä¸€ä¸ªè¾“å‡º

            return output

        elif self.mode == 'zero_shot':
            # ä½¿ç”¨zero_shotæ¨¡å¼
            output = None
            for i, j in enumerate(self.cosyvoice.inference_zero_shot(
                text,
                prompt_text,
                self.prompt_audio if self.prompt_audio else './asset/zero_shot_prompt.wav',
                stream=False
            )):
                output = j['tts_speech']
                break

            return output

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {self.mode}ï¼Œè¯·ä½¿ç”¨ 'instruct2' æˆ– 'zero_shot'")


def prepare_reference_audio():
    """å‡†å¤‡å‚è€ƒéŸ³é¢‘çš„è¾…åŠ©å‡½æ•°"""
    ref_dir = Path('./reference_audio')
    ref_dir.mkdir(exist_ok=True)

    print("\n" + "="*60)
    print("å‡†å¤‡å‚è€ƒéŸ³é¢‘")
    print("="*60)
    print("\nä¸ºäº†ç”Ÿæˆå…·æœ‰'å°O'éŸ³è‰²çš„éŸ³é¢‘ï¼Œä½ éœ€è¦ï¼š")
    print("\n1. å½•åˆ¶ä¸€æ®µ'å°O'çš„éŸ³é¢‘ï¼ˆ10-30ç§’ï¼‰")
    print("   - å†…å®¹ï¼šéšä¾¿è¯´å‡ å¥è¯ï¼Œè‡ªç„¶å³å¯")
    print("   - è´¨é‡ï¼šæ¸…æ™°ã€æ— å™ªéŸ³ã€æ— èƒŒæ™¯éŸ³ä¹")
    print("   - æ ¼å¼ï¼šWAV æˆ– MP3")
    print("\n2. å°†éŸ³é¢‘ä¿å­˜ä¸º: ./reference_audio/prompt.wav")
    print("\n3. ç¤ºä¾‹å½•éŸ³å†…å®¹:")
    print("   'ä½ å¥½ï¼Œæˆ‘æ˜¯å°Oã€‚å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšå¾—æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚'")
    print("\nå¦‚æœæ²¡æœ‰å‚è€ƒéŸ³é¢‘ï¼Œå°†ä½¿ç”¨é»˜è®¤éŸ³è‰²ã€‚")
    print("="*60 + "\n")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ä½¿ç”¨CosyVoice3æ‰¹é‡ç”Ÿæˆè®­ç»ƒéŸ³é¢‘')
    parser.add_argument('--script', default='./data/training_complete.json',
                       help='è®­ç»ƒæ–‡æ¡ˆJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', default='digital_human_training_data',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model', default='pretrained_models/Fun-CosyVoice3-0.5B',
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--prompt', default='./reference_audio/prompt.wav',
                       help='å‚è€ƒéŸ³é¢‘è·¯å¾„')
    parser.add_argument('--mode', default='instruct2', choices=['instruct2', 'zero_shot'],
                       help='ç”Ÿæˆæ¨¡å¼: instruct2(æ¨è) æˆ– zero_shot')
    parser.add_argument('--prepare-ref', action='store_true',
                       help='æ˜¾ç¤ºå¦‚ä½•å‡†å¤‡å‚è€ƒéŸ³é¢‘çš„è¯´æ˜')

    args = parser.parse_args()

    # å¦‚æœç”¨æˆ·è¯·æ±‚å‡†å¤‡å‚è€ƒéŸ³é¢‘çš„è¯´æ˜
    if args.prepare_ref:
        prepare_reference_audio()
        exit(0)

    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è¿è¡Œ
    generator = CosyVoice3Generator(
        output_dir=args.output,
        model_dir=args.model,
        prompt_audio=args.prompt,
        mode=args.mode
    )

    metadata = generator.generate_training_set(script_json_path=args.script)

    print("\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"   æ€»å¥æ•°: {len(metadata)}")

    # æŒ‰sectionç»Ÿè®¡
    from collections import Counter
    sections = Counter([m['section'] for m in metadata])
    print(f"   Sectionæ•°: {len(sections)}")
    print(f"\nå‰10ä¸ªsection:")
    for section, count in list(sections.items())[:10]:
        print(f"     {section}: {count}å¥")
